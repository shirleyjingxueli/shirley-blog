# 搜索引擎与 SEO
  [谷歌 SEO 优化指南](https://developers.google.com/search/docs/fundamentals/seo-starter-guide?hl=zh-cn) 

  [百度 SEO 优化指南](https://www.dragonmetrics.cn/baidu-seo-guide/)

  [SEO 优化具体内容](https://juejin.cn/post/6844903824428105735)

## 搜索引擎核心
  **搜索引擎的核心系统包含三部分：爬虫系统、索引系统 与 检索系统**
  ![搜索引擎核心系统](./image/search_engine.png)
### 爬虫系统
  **搜索引擎通过高性能的爬虫系统来完成持续的网页抓取，并且将抓取到的网页存入存储平台中。**一般来说，可以将主渠道的网页存放在基于 LSM 树的 HBase 中，以便支持数据的高效读写。

#### 爬虫系统是如何工作的？
  爬虫系统是一种自动化程序，用于浏览互联网上的网页并收集信息。它主要包括以下几个步骤：

  1. **确定起始点：**爬虫系统首先需要确定一个或多个起始点（种子URL），从这些起始点开始抓取网页。这些起始点可以是预定义的，也可以是通过搜索引擎或其他方式获取的。
  2. **抓取网页：**爬虫系统根据起始点的链接，访问网页并获取网页的内容。它使用HTTP请求来获取网页，并获得网页的HTML代码。
  3. **解析网页：**爬虫系统解析网页的HTML代码，提取出页面中的链接、文本内容、图片、视频等元素。它使用各种技术，如正则表达式、DOM解析等，来识别和提取所需的信息。
  4. **存储数据：**爬虫系统将抓取到的数据存储在数据库或其他数据存储系统中。这些数据可以包括网页的内容、链接、标题、描述、关键词等信息，以及其他自定义的数据。
  5. **跟踪链接：**爬虫系统从当前抓取的网页中提取出的链接作为下一步的目标，继续抓取这些链接指向的网页。这样，爬虫系统通过不断跟踪链接，逐步扩展其抓取的范围。
  6. **控制爬取速度：**为了避免对网站造成过大的负载或被视为恶意爬取，爬虫系统通常会设置爬取速度限制。这可以通过延时请求、设置访问频率等方式来实现。
  7. **处理重复和异常情况：**在抓取过程中，可能会遇到重复的链接或异常情况，如页面不存在或服务器错误。爬虫系统需要处理这些情况，避免重复抓取相同的页面，以及记录和处理异常情况。
  8. **深度优先或者广度优先抓取：**爬虫系统可以采用深度优先或广度优先的抓取策略。深度优先抓取意味着优先抓取链接的深层页面，而广度优先抓取意味着优先抓取链接的相邻页面。这取决于具体的抓取需求和目标。

  **总体而言，爬虫系统通过自动化地浏览、抓取和解析网页，收集信息并存储在数据库中。它可以帮助搜索引擎建立索引、数据挖掘、信息抓取和其他应用。然而，需要注意的是，爬虫系统必须遵守网站的爬取规则和法律法规，尊重网站的隐私和版权等相关权益。**

### 索引系统
  在爬虫系统抓取到网页之后，需要对这些网页进行一系列的处理，它们才可以变成可用的索引。 **处理分为两个阶段，首先对网页进行预处理，主要的手段包括相似网页去重、网页质量分析、分词处理等工作，然后是对网页进行反作弊的分析工作，来避免一些作弊网页干扰搜索结果。**    

  处理好网页之后，我们就要为搜索引擎生成索引，索引的生成过程主要分为三步。

  - **第一步，索引拆分。**

    由于抓取到的网页量级非常大，把它们全部生成索引不太现实，因此**会在离线阶段，根据之前的网页预处理结果，进行计算和筛选，分别分离出高质量和普通质量的网页集合。**这样就可以进行**分层索引**了。由于网页集合还是很多，因此还需要进行基于文档的拆分，以便生成索引。

  - **第二步，索引构建。**

    在确认索引分片机制后，可以使用 Map Reduce 服务，来为每个索引分片生成对应的任务，然后生成相应的倒排索引文件。每个倒排索引文件代表一个索引分片，它们都可以加载到线上的服务器中，来提供检索服务。

  - **第三步，索引更新。**

    为了保证能实时更新数据，搜索引擎会使用全量搜索结合增量索引的机制来完成索引更新。并且由于搜索引擎的全量索引数据量巨大，因此，一般使用**滚动合并法**来完成索引更新。

  有了创建出来的索引之后，搜索引擎就可以为万亿级别的网页提供高效的检索服务了。

### 检索系统
  
  在检索阶段，如果用户搜索了一个关键词，那么搜索引擎首先需要做查询分析，也就是通过分析查询词本身以及用户行为特征，找出用户的真实查询意图。如果发现查询词有误或者结果很少，搜索引擎还会进行拼写更正或者相关查询推荐，然后再以改写后的查询词去检索服务中的查询结果。

  在检索服务中，搜索引擎会将查询词发送给相应的索引分片，索引分片通过倒排索引的检索机制，将自己所负责的分片结果返回。对于返回的结果，搜索引擎再根据相关性分析和质量分析，使用机器学习进行打分，选出 Top K 个结果来完成检索。

  与广告引擎和推荐引擎相比，**搜索引擎最大的特点，就是它有一个很强的检索体约束条件，那就是用户输入的查询词。可以说，查询词是搜索引擎进行检索的最核心的信息。**
  
### 搜索引擎是如何进行查询分析的

  ![搜索引擎查询分析](search_engine_token.png) 

### 搜索引擎是如何进行查询纠错的
  ![搜索引擎查询纠错](search_engine_error.png)

## SEO
### 简介
  SEO（Search Engine Optimization），即搜索引擎优化， SEO 是随着搜索引擎的出现而来的，两者是相互促进，互利共生的关系。**SEO 的存在就是为了提升网页在搜索引擎自然搜索结果中的收录数量以及排序位置而做的优化行为。而优化的目的是为了提升网站在搜索引擎中的权重，增加对搜索引擎的优化度，使得用户在访问网站时能够排名在前面。**

  分类：白帽 SEO 和黑帽 SEO。白帽 SEO，起到了改良和规范网站设计的作用，使网站对搜索引擎和用户更加友好，并且网站也能从搜索引擎中获取合理的流量，这是搜索引擎鼓励和支持的。黑帽 SEO，利用和放大搜索引擎政策缺陷来获取更多用户的访问量，这类行为大多是欺骗搜索引擎，一般搜索引擎公司是不支持与鼓励的。

### SEO 能做什么
  1. 对网站的标题、关键字、描述精心设置，反映网站的定位，让搜索引擎明白网站是做什么的；

  2. 网站内容优化：内容与关键字的对应，增加关键字的密度；

  3. 在网站上合理设置 Robots.txt 文件；

  4. 生成针对搜索引擎友好的网站地图；

  5. 增加外部链接，到各个网站上宣传。
## 前端 SEO 规范
  **基于搜索引擎的工作原理，前端 SEO 主要基于三个维度：网站结构优化，网页代码优化，前端性能优化。**

### 网站结构布局优化
  - 控制首页链接数量
  - 扁平化的目录层次
  - 导航优化
  - 网站的结构布局
  - 版权信息和友情链接
  - 利用布局，把重要内容 HTML 代码放在最前面
  - 控制页面的大小，减少 http 请求，提高网站的加载速度

#### 1.1 控制首页链接数量和 SEO 关系？
  **控制首页链接数量是为了提供良好的用户体验、设计合理的导航结构、优化页面权重分配和提高索引效率。需要根据具体的网站类型、目标受众和优化策略来权衡和控制首页链接数量，以获得最佳的用户体验和搜索引擎优化效果。**

  **1. 用户体验：** 首页是用户进入网站的主要入口之一，一个清晰、简洁的首页可以提供良好的用户体验。如果首页链接数量过多，会使页面过于拥挤和混乱，给用户带来困扰，降低用户的满意度和导航效果。通过控制首页链接数量，可以使页面更易于浏览和导航，提高用户的满意度和留存率。

  **2.导航结构和层级：**首页链接数量的控制有助于设计合理的网站导航结构。一个好的导航结构可以帮助用户更轻松地找到所需的内容，并帮助搜索引擎理解网站的层级结构。通过精心选择和组织首页链接，可以提供清晰的导航路径，使用户和搜索引擎更容易理解和浏览网站。

  **3.页面权重分配：**搜索引擎在分配页面权重时，通常会考虑页面上的链接数量和链接权重分配。首页通常是网站中最重要的页面之一，它具有更高的权重。过多的链接数量可能导致首页的权重分散，降低了首页对其他重要页面的权重传递。通过控制首页链接数量，可以更好地管理和分配页面的权重，提高关键页面的排名和可见性。

  **4.索引效率：**搜索引擎爬虫在抓取和索引网页时需要消耗资源和时间。如果首页链接数量过多，爬虫需要处理更多的链接和页面，这可能导致爬取速度变慢，或者无法完全抓取和索引所有页面。通过控制首页链接数量，可以帮助搜索引擎更有效地抓取和索引网站的重要页面，提高整体的索引效率。

#### 1.2 扁平化目录层次 和 SEO 关系？
  在 SEO 优化中，扁平化目录结构是一种常见的优化技术，它将网站的目录结构设计为较为扁平的形式，即减少目录层级的嵌套，将页面尽可能地放置在更浅的目录层级下。这样做的主要原因有以下几点：

  1. **提升页面的可访问性：** 扁平化的目录结构可以使页面更容易被搜索引擎爬虫和用户找到。搜索引擎爬虫通过跟踪链接来抓取页面，较浅的目录层级意味着更少的点击次数和更短的路径，使爬虫能够更快速地到达目标页面。对于用户来说，扁平化的目录结构也更加直观，使他们能够更轻松地浏览和访问所需的页面。

  2. **提高页面权重分配：** 搜索引擎在分配页面权重时，通常会考虑页面所在目录层级的深度。较深的目录层级可能会被搜索引擎认为是次要内容，分配的权重较低。而扁平化的目录结构可以使页面更接近站点的根目录，权重分配更有利于页面的排名和可见性。

  3. **简化 URL 结构：** 扁平化的目录结构可以产生简洁、易读的 URL。简短、描述性的 URL 对搜索引擎和用户来说更具吸引力，有助于提高点击率和用户体验。相比较深层次的目录结构，扁平化的 URL 结构更容易被记忆和分享。

  4. **降低维护复杂性：** 较复杂的目录结构可能导致网站维护的复杂性增加。在扁平化的目录结构中，页面的位置更清晰明确，更容易进行调整和管理。这对于网站的更新、重构或重组时，减少了开发人员的工作量和维护成本。

  需要注意的是，扁平化目录结构并不适用于所有类型的网站。对于某些网站，如电子商务网站或新闻门户，较深的目录结构可能是不可避免的。在进行目录结构设计时，应根据网站的特点和需求进行权衡，并确保用户体验和内容组织的合理性。

#### 1.3 版权信息和 SEO 关系？

  **控制版权信息和友情链接是为了维护网站的合法性、信誉度和用户体验。它们是 SEO 优化中重要的方面，能够帮助网站建立良好的形象，提高搜索引擎排名和用户满意度。**

  1. **合法性和知识产权保护：** 版权信息的控制是为了确保网站内容的合法性和知识产权的保护。通过在网站上提供正确的版权声明和相关信息，可以表明网站的内容是合法的，并遵守版权法律。这有助于建立网站的信誉度，防止侵权行为，并避免法律纠纷。

  2. **信誉度和可信度：** 友情链接的控制是为了维护网站的信誉度和可信度。友情链接是指与其他网站的互相链接，用于推荐和引导用户浏览其他相关内容。通过选择合适的友情链接，并确保这些链接指向优质和相关的网站，可以提升网站的信誉度和可信度。而过多或低质量的友情链接可能会降低网站的信誉度，甚至被搜索引擎视为垃圾链接或操纵排名的行为。

  3. **用户体验和导航结构：** 友情链接的控制也与用户体验和导航结构密切相关。通过精心选择和组织友情链接，可以提供有用的资源和相关内容给用户，增强网站的导航功能和用户体验。然而，过多的友情链接可能会使页面过于拥挤和混乱，降低用户的满意度和导航效果。因此，需要控制友情链接的数量和质量，以提供更好的用户体验。

#### 1.4 控制页面的大小，减少 http请求，提高网站的加载速度和 SEO 关系？

  **改善用户体验和搜索引擎的爬取效果**

  通过控制页面大小、减少 HTTP 请求和提高网站加载速度，可以改善用户体验、降低跳出率，同时提高搜索引擎的爬取效果，从而增加网站的可访问性和排名。这些优化措施是前端 SEO 中的重要方面，**可以为网站带来更多的有机流量和更好的用户参与度。**

  1. **提高用户体验：** 用户更倾向于访问加载速度快的网站，而加载速度慢的网站可能导致用户流失率增加。通过控制页面大小和减少 HTTP 请求，可以减少网页的加载时间，提高用户的访问体验，使用户能够更快地浏览网页内容。

  2. **降低跳出率：** 跳出率是指用户只访问一个页面后离开网站的比例。如果网页加载时间过长，用户可能会感到不耐烦并离开。通过优化页面加载速度，减少等待时间，可以降低跳出率，增加用户停留时间和页面浏览量。

  3. **改善搜索引擎爬取效果：** 搜索引擎的爬虫会访问网页并分析其中的内容，以确定网页的相关性和排名。如果网页加载速度慢，搜索引擎爬虫可能无法完全抓取网页内容，从而影响搜索引擎对网页的理解和索引。通过控制页面大小和减少 HTTP 请求，可以提高网页的加载速度，使搜索引擎能够更好地爬取和理解网页内容，从而有利于网页的排名和可见性。

  4. **移动设备优化：**在移动设备上，网络连接速度可能较慢，用户对加载速度的敏感度更高。通过优化页面大小和减少 HTTP 请求，可以减少移动设备上的数据传输量，加快页面加载速度，提供更好的移动用户体验。

#### 搜索引擎可能无法完全抓取网页内容的原因？

  **搜索引擎爬虫无法完全抓取网页内容可能是因为动态内容、登录和权限限制、链接限制、速度限制、Robots.txt 文件规则以及网络问题等原因。为了确保网页内容被搜索引擎爬虫完整抓取，开发人员可以采取一些措施，如提供静态 HTML 版本的页面、合理配置 Robots.txt 文件、优化网站的速度和性能等。**

  1. **动态内容：** 某些网页包含动态生成的内容，这些内容可能是通过 JavaScript 或 AJAX 等技术在客户端生成的。搜索引擎爬虫在抓取网页时可能无法执行 JavaScript 或处理 AJAX 请求，因此无法获取到动态生成的内容。

  2. **登录和权限限制：** 某些网站可能要求用户登录或有权限限制才能访问特定的内容。搜索引擎爬虫通常以匿名用户身份访问网页，因此无法获取到需要登录或有权限限制的内容。

  3. **链接限制：** 搜索引擎爬虫是通过跟踪链接来抓取网页的，如果某些链接无法被爬虫访问到，那么相应的内容也无法被抓取。例如，如果网页上的链接使用 JavaScript 生成或需要登录才能访问，搜索引擎爬虫可能无法跟踪到这些链接，导致内容无法被完全抓取。

  4. **速度限制：**为了保护网站的服务器资源和防止恶意爬取，网站可能会对爬虫设置访问速度限制或访问频率限制。如果爬虫访问网站过于频繁或超过了速度限制，网站可能会拒绝响应或返回错误内容，导致爬虫无法完全抓取网页内容。

  5. **Robots.txt 文件：** 网站的 robots.txt 文件是用来指示搜索引擎爬虫哪些页面可以抓取和哪些页面不应该抓取的。如果网站的 robots.txt 文件中禁止了某些页面的抓取，搜索引擎爬虫将遵守这些规则，导致相应页面的内容无法被抓取。

  6. **网络问题：** 有时候，网络连接可能不稳定或存在其他问题，导致搜索引擎爬虫无法正常访问网页或获取完整的内容。

### 网页代码优化
  - 突出重要内容： 合理的 TDK 信息

    标题：只强调重点即可，尽量把重要的关键词放在前面，关键词不要重复出现，尽量做到每个页面的```<title>```标题中不要设置相同的内容。

    标签：关键词，列举出几个页面的重要关键字即可，切记过分堆砌。

    标签：网页描述，需要高度概括网页内容，切记不能太长，过分堆砌关键词，每个页面也要有所不同。

  - 语义化书写 HTML 代码，符合 W3C 标准。 

    尽量让代码语义化，在适当的位置使用适当的标签，用正确的标签做正确的事。让阅读源码者和“蜘蛛”都一目了然。比如：h1-h6 是用于标题类的，

    标签是用来设置页面主导航，列表形式的代码使用 ul 或 ol，重要的文字使用 strong 等。

    #### 标签

      * 页内链接，要加 “title” 属性加以说明，让访客和 “蜘蛛” 知道。而外部链接，链接到其他网站的，则需要加上 el="nofollow" 属性, 告诉 “蜘蛛” 不要爬，因为一旦“蜘蛛”爬了外部链接之后，就不会再回来了。
      * 正文标题要用标签：h1 标签自带权重“蜘蛛” 认为它最重要，一个页面有且最多只能有一个 h1 标签，放在该页面最重要的标题上面，如首页的 logo 上可以加 h1 标签。副标题用 h2 标签, 而其它地方不应该随便乱用 h 标题标签。
      * img 标签应该使用 alt 属性加以说明
      * 表格应该使用 caption 表格标题标签
      * br 标签只用于文本内容的换行
      * strong、em标签：**需要强调时使用。**strong 标签在搜索引擎中能够得到高度的重视，它能突出关键词，表现重要的内容，em 标签强调效果仅次于 strong 标签；b、i 标签：只是用于显示效果时使用，在 SEO 中不会起任何效果。

    #### 文本缩进不要使用特殊符号
    #### 重要内容不要使用 JS 输出
    #### 尽量少使用 iframe 框架，因为“蜘蛛”一般不会读取其中的内容。
    #### 谨慎使用 display:none

### 前端性能优化
  - 减少 http 请求数量
  - 控制资源文件加载优先级
  - 尽量外链 CSS 和 JS
  - 利用浏览器缓存
  - 减少重排
  - 减少 DOM 操作
  - 图标使用 IconFont 替换
  - 不要使用 CSS 表达式，会影响效率
  - 使用 CDN 网格缓存，加快用户的访问速度，减轻服务器压力
  - 使用 GZIP 压缩，浏览速度变快，搜索引擎的蜘蛛抓取信息量也会加大
  - 伪静态设置

## SEO 优化
### 具体优化方式
- 网站的正常运行时间
- 网站的年龄，网页内容的新鲜程度，好的原创内容是最好的优化方式
- 网站采用 HTTPS 
- HTML 代码的质量，是否存在错误
- 网站在站点访问的深度。

具体：
- 添加关键字：TDK, og相关信息
- HTML 语义化标签
- 网站根目录方式 robots.txt 告知哪些内容是可以爬取的
- 连接是否进行爬取 比如设置 nofollow
- 构建 sitemap，可以让搜索引擎有目的和有条件的扫描和爬取页面数据。
- 将网站链接放置在一些本不该进行外链的其他网站上
- 将其他网站的内容拷贝过来，欺骗搜索引擎。

### robots.txt 和 sitemap 的区别？
  1. Robots.txt（机器人协议文件）：

  Robots.txt是一个位于网站根目录下的文本文件，用于向搜索引擎的爬虫（机器人）指示哪些页面可以被抓取和索引，哪些页面应该被忽略。它是一种指导搜索引擎爬虫行为的协议文件。
  作用：通过在Robots.txt文件中设置规则，网站管理员可以控制搜索引擎爬虫的访问权限，以防止爬取敏感或无关的页面，并指示搜索引擎按照网站的需求进行抓取和索引。
  格式：Robots.txt文件使用简单的语法规则，可包含针对特定爬虫的指令，如User-agent和Disallow等。

  2. Sitemap（站点地图）：

  Sitemap是一个XML文件，用于提供搜索引擎关于网站中所有可抓取页面的结构和信息。它列出了网站的URL和其他相关元数据，如最后更新时间、页面优先级等。
  作用：Sitemap帮助搜索引擎更好地理解网站的结构，加速抓取和索引网站的内容，提供更全面和准确的搜索结果。
  格式：Sitemap文件遵循特定的XML格式，使用标签和元素来描述网站中的页面和相关信息。

  **总结：**

  **Robots.txt和Sitemap在SEO中扮演不同的角色。Robots.txt是一种协议文件，告诉搜索引擎爬虫可以访问和抓取哪些页面，而Sitemap是一个XML文件，提供搜索引擎关于网站结构和内容的信息。Robots.txt用于控制爬虫的访问权限，而Sitemap则帮助搜索引擎更好地了解网站的结构和内容，优化抓取和索引过程。**

## SEM
### 简介
  SEM，即搜索引擎营销（Search Engine Marketing）

  SEM 一般指竞价推广，在搜索引擎后台账户投钱，使广告获取相关的排名，一般搜索页面上会展现“广告”两个词，SEO（Search Engine Optimization，即搜索引擎优化）是一种技术手段，对网站进行有针对性的优化，提高网站在搜索引擎中的自然排名，吸引更多的用户访问网站，提高网站的访问量。

  SEM：见效快，只要开始投放，效果立马可见，反之亦然。

  SEO：是一个长期优化，逐步完善的过程，见效慢。如果停止 SEO 优化，效果也可以持续半年左右


    
## 面试题
### 搜索引擎的工作原理
  搜索引擎的核心主要包含了爬虫系统、索引系统 和 检索系统。

  具体的工作流程：

  1. 爬虫系统-爬取网页

    搜索引擎会使用爬虫程序从互联网上抓取网页内容，并将其存储在数据库中。

  2. 索引系统-网页预处理
    
    拿到网页之后，为了方便构建索引，搜索引擎会对网页进行一些预处理，比如网页去重，网页质量分析，分词处理等。

  3. 索引系统-构建索引
    
    搜索引擎会对抓取到的网页进行索引，建立一个包含关键词和网页链接的索引数据库，这样当用户输入关键词时，搜索引擎就可以快速的找到相关的网页。

  4. 检索系统-用户查询

    当用户输入关键词进行搜索时，搜索引擎会对用户的查询进行处理，包括分词、去除停用词等操作，以便更好的理解用户的意图。

  5. 检索系统-检索与排序
    
    搜索引擎会根据用户的查询，在索引数据库中查找包含相关关键词的网页，并根据一定的算法对这些网页进行排序，以便将相关的网页展示给用户。

  6. 结果展示
    
    搜索引擎会将排序后的网页结果展示给用户，通常以列表的形式呈现，并提供相关的标题、摘要和链接等信息，以便用户选择点击查看。

  7. 网页重排

    搜索引擎会根据用户的点击行为和反馈信息，对搜索结果进行优化和调整，以提供更好的搜索结果。

  具体流程可以参考[搜索引擎核心](#搜索引擎核心)  
